{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Spatial Validation Deep Dive\n",
    "\n",
    "## Objectif\n",
    "Explorer en détail la généralisation spatiale du modèle.\n",
    "C'est le point critique de ce challenge: le modèle sera évalué sur des sites non vus.\n",
    "\n",
    "---\n",
    "\n",
    "## Table des Matières\n",
    "1. [Contexte et Importance](#1-context)\n",
    "2. [Analyse de la Distribution Spatiale](#2-spatial-dist)\n",
    "3. [Stratégies de Validation](#3-strategies)\n",
    "4. [Diagnostic de Généralisation](#4-diagnosis)\n",
    "5. [Amélioration de la Robustesse Spatiale](#5-improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Contexte et Importance <a id='1-context'></a>\n",
    "\n",
    "### Pourquoi la validation spatiale est cruciale?\n",
    "\n",
    "**Problème**: Les mesures d'un même site partagent des caractéristiques non explicites:\n",
    "- Géologie locale\n",
    "- Usages du sol non capturés\n",
    "- Biais de mesure spécifiques au site\n",
    "\n",
    "**Conséquence**: Un modèle entraîné avec validation random peut:\n",
    "- \"Mémoriser\" les sites plutôt qu'apprendre les relations générales\n",
    "- Afficher un score CV excellent mais échouer sur de nouveaux sites\n",
    "\n",
    "**Solution**: Validation Leave-Site-Out ou GroupKFold par site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GroupKFold, LeaveOneGroupOut, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.config import TARGETS, RANDOM_STATE\n",
    "from src.paths import PROCESSED_DATA_DIR, FIGURES_DIR\n",
    "\n",
    "print(\"Setup completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Analyse de la Distribution Spatiale <a id='2-spatial-dist'></a>\n",
    "\n",
    "Comprendre comment les sites sont distribués géographiquement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_site_distribution(df, site_col='site_id', lat_col='latitude', lon_col='longitude'):\n",
    "    \"\"\"\n",
    "    Analyse la distribution des sites.\n",
    "    \"\"\"\n",
    "    # Nombre de sites\n",
    "    n_sites = df[site_col].nunique()\n",
    "    print(f\"Nombre de sites uniques: {n_sites}\")\n",
    "    \n",
    "    # Observations par site\n",
    "    obs_per_site = df.groupby(site_col).size()\n",
    "    print(f\"\\nObservations par site:\")\n",
    "    print(obs_per_site.describe())\n",
    "    \n",
    "    # Distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Histogram des observations par site\n",
    "    obs_per_site.hist(bins=30, ax=axes[0], edgecolor='black')\n",
    "    axes[0].set_xlabel('Nombre d\\'observations')\n",
    "    axes[0].set_ylabel('Nombre de sites')\n",
    "    axes[0].set_title('Distribution des observations par site')\n",
    "    \n",
    "    # Carte des sites\n",
    "    site_coords = df.groupby(site_col)[[lat_col, lon_col]].first()\n",
    "    site_coords['n_obs'] = obs_per_site\n",
    "    \n",
    "    scatter = axes[1].scatter(\n",
    "        site_coords[lon_col], site_coords[lat_col],\n",
    "        c=site_coords['n_obs'], cmap='viridis',\n",
    "        s=50, alpha=0.7\n",
    "    )\n",
    "    plt.colorbar(scatter, ax=axes[1], label='Nombre d\\'observations')\n",
    "    axes[1].set_xlabel('Longitude')\n",
    "    axes[1].set_ylabel('Latitude')\n",
    "    axes[1].set_title('Distribution géographique des sites')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return obs_per_site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse\n",
    "# obs_per_site = analyze_site_distribution(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_target_by_site(df, target, site_col='site_id'):\n",
    "    \"\"\"\n",
    "    Analyse la variabilité de la target par site.\n",
    "    \"\"\"\n",
    "    site_stats = df.groupby(site_col)[target].agg(['mean', 'std', 'min', 'max', 'count'])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Distribution des moyennes par site\n",
    "    site_stats['mean'].hist(bins=30, ax=axes[0], edgecolor='black')\n",
    "    axes[0].axvline(df[target].mean(), color='red', linestyle='--', label='Global mean')\n",
    "    axes[0].set_xlabel(f'Mean {target} par site')\n",
    "    axes[0].set_title('Variabilité inter-sites')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Variance intra-site\n",
    "    site_stats['std'].hist(bins=30, ax=axes[1], edgecolor='black')\n",
    "    axes[1].set_xlabel(f'Std {target} par site')\n",
    "    axes[1].set_title('Variabilité intra-site')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Ratio variance inter/intra\n",
    "    var_inter = site_stats['mean'].var()\n",
    "    var_intra = (site_stats['std']**2).mean()\n",
    "    print(f\"\\nVariance inter-sites: {var_inter:.4f}\")\n",
    "    print(f\"Variance intra-site moyenne: {var_intra:.4f}\")\n",
    "    print(f\"Ratio inter/intra: {var_inter/var_intra:.2f}\")\n",
    "    print(\"(Ratio élevé = les sites sont très différents entre eux)\")\n",
    "    \n",
    "    return site_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse pour chaque target\n",
    "# for target in TARGETS:\n",
    "#     print(f\"\\n=== {target} ===\")\n",
    "#     analyze_target_by_site(train, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Stratégies de Validation <a id='3-strategies'></a>\n",
    "\n",
    "### Comparaison des approches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_cv_strategies(model, X, y, groups):\n",
    "    \"\"\"\n",
    "    Compare différentes stratégies de validation croisée.\n",
    "    \n",
    "    Logique:\n",
    "    - Random CV: sous-estime l'erreur de généralisation\n",
    "    - GroupKFold: estimation réaliste\n",
    "    - LeaveOneGroupOut: le plus conservateur\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Random 5-fold CV\n",
    "    print(\"1. Random 5-fold CV (ATTENTION: trop optimiste!)\")\n",
    "    random_cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = []\n",
    "    for train_idx, val_idx in random_cv.split(X):\n",
    "        model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "        pred = model.predict(X.iloc[val_idx])\n",
    "        scores.append(np.sqrt(mean_squared_error(y.iloc[val_idx], pred)))\n",
    "    results['random'] = {'mean': np.mean(scores), 'std': np.std(scores)}\n",
    "    print(f\"   RMSE: {results['random']['mean']:.4f} ± {results['random']['std']:.4f}\")\n",
    "    \n",
    "    # 2. Group 5-fold CV\n",
    "    print(\"\\n2. Spatial GroupKFold (5 folds)\")\n",
    "    group_cv = GroupKFold(n_splits=5)\n",
    "    scores = []\n",
    "    for train_idx, val_idx in group_cv.split(X, y, groups):\n",
    "        model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "        pred = model.predict(X.iloc[val_idx])\n",
    "        scores.append(np.sqrt(mean_squared_error(y.iloc[val_idx], pred)))\n",
    "    results['group_5fold'] = {'mean': np.mean(scores), 'std': np.std(scores)}\n",
    "    print(f\"   RMSE: {results['group_5fold']['mean']:.4f} ± {results['group_5fold']['std']:.4f}\")\n",
    "    \n",
    "    # 3. Leave-One-Site-Out (si pas trop de sites)\n",
    "    n_sites = len(np.unique(groups))\n",
    "    if n_sites <= 50:  # Limiter pour le temps de calcul\n",
    "        print(f\"\\n3. Leave-One-Site-Out ({n_sites} sites)\")\n",
    "        logo_cv = LeaveOneGroupOut()\n",
    "        scores = []\n",
    "        for train_idx, val_idx in logo_cv.split(X, y, groups):\n",
    "            model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "            pred = model.predict(X.iloc[val_idx])\n",
    "            scores.append(np.sqrt(mean_squared_error(y.iloc[val_idx], pred)))\n",
    "        results['logo'] = {'mean': np.mean(scores), 'std': np.std(scores)}\n",
    "        print(f\"   RMSE: {results['logo']['mean']:.4f} ± {results['logo']['std']:.4f}\")\n",
    "    \n",
    "    # Résumé\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RÉSUMÉ: Écart entre Random et Spatial CV\")\n",
    "    gap = results['group_5fold']['mean'] - results['random']['mean']\n",
    "    print(f\"Écart: {gap:.4f} ({100*gap/results['random']['mean']:.1f}% d'augmentation)\")\n",
    "    print(\"Un écart important indique un risque d'overfitting spatial!\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "# cv_comparison = compare_cv_strategies(model, X_train, y_train[TARGETS[0]], site_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Diagnostic de Généralisation <a id='4-diagnosis'></a>\n",
    "\n",
    "Identifier les sites/régions où le modèle performe mal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_spatial_performance(y_true, y_pred, site_ids, coords_df=None):\n",
    "    \"\"\"\n",
    "    Diagnostique la performance par site.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Valeurs réelles\n",
    "        y_pred: Prédictions (OOF)\n",
    "        site_ids: Identifiants de site\n",
    "        coords_df: DataFrame avec lat/lon par site (optionnel)\n",
    "    \"\"\"\n",
    "    # Performance par site\n",
    "    df = pd.DataFrame({\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'residual': y_true - y_pred,\n",
    "        'site_id': site_ids\n",
    "    })\n",
    "    \n",
    "    site_perf = df.groupby('site_id').apply(lambda x: pd.Series({\n",
    "        'rmse': np.sqrt(mean_squared_error(x['y_true'], x['y_pred'])),\n",
    "        'mae': np.abs(x['residual']).mean(),\n",
    "        'bias': x['residual'].mean(),\n",
    "        'n_obs': len(x)\n",
    "    })).reset_index()\n",
    "    \n",
    "    # Visualisation\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Distribution du RMSE par site\n",
    "    site_perf['rmse'].hist(bins=20, ax=axes[0, 0], edgecolor='black')\n",
    "    axes[0, 0].axvline(site_perf['rmse'].median(), color='red', linestyle='--', label='Median')\n",
    "    axes[0, 0].set_xlabel('RMSE par site')\n",
    "    axes[0, 0].set_title('Distribution de la performance par site')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Distribution du biais\n",
    "    site_perf['bias'].hist(bins=20, ax=axes[0, 1], edgecolor='black')\n",
    "    axes[0, 1].axvline(0, color='red', linestyle='--')\n",
    "    axes[0, 1].set_xlabel('Biais moyen par site')\n",
    "    axes[0, 1].set_title('Distribution du biais (positive = sous-estimation)')\n",
    "    \n",
    "    # 3. RMSE vs nombre d'observations\n",
    "    axes[1, 0].scatter(site_perf['n_obs'], site_perf['rmse'], alpha=0.6)\n",
    "    axes[1, 0].set_xlabel('Nombre d\\'observations par site')\n",
    "    axes[1, 0].set_ylabel('RMSE')\n",
    "    axes[1, 0].set_title('Performance vs Taille du site')\n",
    "    \n",
    "    # 4. Sites problématiques\n",
    "    top_worst = site_perf.nlargest(10, 'rmse')\n",
    "    axes[1, 1].barh(range(len(top_worst)), top_worst['rmse'])\n",
    "    axes[1, 1].set_yticks(range(len(top_worst)))\n",
    "    axes[1, 1].set_yticklabels(top_worst['site_id'])\n",
    "    axes[1, 1].set_xlabel('RMSE')\n",
    "    axes[1, 1].set_title('Top 10 sites les plus difficiles')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiques\n",
    "    print(\"\\n=== Statistiques de Performance par Site ===\")\n",
    "    print(f\"RMSE médian: {site_perf['rmse'].median():.4f}\")\n",
    "    print(f\"RMSE moyen: {site_perf['rmse'].mean():.4f}\")\n",
    "    print(f\"Écart-type RMSE: {site_perf['rmse'].std():.4f}\")\n",
    "    print(f\"\\nSites avec biais > 0 (sous-estimation): {(site_perf['bias'] > 0).sum()} / {len(site_perf)}\")\n",
    "    \n",
    "    return site_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic avec les prédictions OOF\n",
    "# site_perf = diagnose_spatial_performance(\n",
    "#     y_train[TARGETS[0]].values,\n",
    "#     lgb_results[TARGETS[0]]['oof_predictions'],\n",
    "#     site_ids\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Amélioration de la Robustesse Spatiale <a id='5-improvement'></a>\n",
    "\n",
    "### Stratégies pour améliorer la généralisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratégie 1: Features spatiales généralisables\n",
    "\"\"\"\n",
    "Au lieu d'utiliser des coordonnées brutes, utiliser:\n",
    "- Altitude, pente, orientation\n",
    "- Distance à des features géographiques (côte, rivière principale)\n",
    "- Caractéristiques du bassin versant\n",
    "- Clusters géographiques (mais attention à la granularité)\n",
    "\"\"\"\n",
    "\n",
    "# Stratégie 2: Régularisation forte\n",
    "\"\"\"\n",
    "- Augmenter reg_alpha et reg_lambda\n",
    "- Réduire la profondeur des arbres\n",
    "- Augmenter min_child_samples\n",
    "- Réduire num_leaves\n",
    "\"\"\"\n",
    "\n",
    "# Stratégie 3: Ensemble de modèles spatiaux\n",
    "\"\"\"\n",
    "- Entraîner un modèle par région géographique\n",
    "- Combiner avec un modèle global\n",
    "- Pondérer selon la distance aux sites d'entraînement\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_aware_prediction(models, X_train, X_test, train_coords, test_coords, k=5):\n",
    "    \"\"\"\n",
    "    Prédiction pondérée par la proximité spatiale.\n",
    "    \n",
    "    Logique:\n",
    "    - Pour chaque point de test, trouver les k sites d'entraînement les plus proches\n",
    "    - Pondérer les prédictions des modèles entraînés sur ces sites\n",
    "    \n",
    "    Note: Approche expérimentale, à tester.\n",
    "    \"\"\"\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    \n",
    "    # Trouver les voisins spatiaux\n",
    "    nn = NearestNeighbors(n_neighbors=k)\n",
    "    nn.fit(train_coords)\n",
    "    distances, indices = nn.kneighbors(test_coords)\n",
    "    \n",
    "    # Poids inversement proportionnels à la distance\n",
    "    weights = 1 / (distances + 1e-6)\n",
    "    weights = weights / weights.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Prédictions pondérées\n",
    "    # (À adapter selon la structure des modèles)\n",
    "    \n",
    "    return weights, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusions\n",
    "\n",
    "### Checklist Validation Spatiale\n",
    "\n",
    "- [ ] Utiliser GroupKFold ou Leave-Site-Out (JAMAIS random CV seul)\n",
    "- [ ] Comparer les scores random vs spatial pour détecter l'overfitting\n",
    "- [ ] Analyser la performance par site pour identifier les régions difficiles\n",
    "- [ ] Vérifier que les features spatiales sont généralisables\n",
    "- [ ] Ne PAS utiliser site_id ou coordonnées brutes comme features\n",
    "- [ ] Préférer des features dérivées (altitude, distance, clusters larges)\n",
    "- [ ] Régulariser suffisamment le modèle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
