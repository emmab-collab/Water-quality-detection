{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "616441fc-62fd-43c9-961d-5b4606d02a3a",
   "metadata": {},
   "source": [
    "# TerraClimate V2 - Extraction avec Agrégations Temporelles\n",
    "\n",
    "## Améliorations par rapport à V1\n",
    "\n",
    "**V1** : Extrait uniquement la valeur du mois de mesure\n",
    "\n",
    "**V2** : Ajoute des **agrégations temporelles** pour capturer l'historique climatique :\n",
    "\n",
    "| Feature | Description | Pourquoi c'est utile |\n",
    "|---------|-------------|---------------------|\n",
    "| `ppt` | Précipitations du mois | Valeur actuelle |\n",
    "| `ppt_lag1` | Précipitations mois-1 | Ruissellement différé |\n",
    "| `ppt_lag2` | Précipitations mois-2 | Impact retardé |\n",
    "| `ppt_sum3` | Cumul 3 derniers mois | Saturation du sol |\n",
    "| `ppt_anomaly` | Écart à la moyenne saisonnière | Mois exceptionnellement pluvieux/sec |\n",
    "\n",
    "## Note importante\n",
    "\n",
    "TerraClimate est **mensuel** (pas journalier). Pour des cumuls 3j/7j, il faudrait utiliser ERA5 ou CHIRPS.\n",
    "Ici on fait des **lags mensuels** qui capturent quand même l'historique climatique récent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dd7a3f-872e-4e04-8267-5d2c1ef4bcf4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Étape 1 : Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba9c9092-a1e5-410b-95cd-568e8bcca686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: uv in c:\\users\\manu\\appdata\\roaming\\python\\python313\\site-packages (0.9.18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\u001b[2mUsing Python 3.13.1 environment at: c:\\Program Files\\Python313\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m194 packages\u001b[0m \u001b[2min 3.82s\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m pywinpty \u001b[2m(2.0MiB)\u001b[0m\n",
      " \u001b[36m\u001b[1mDownloaded\u001b[0m\u001b[39m pywinpty\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 359ms\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[31merror\u001b[39m\u001b[0m: Failed to install: pywinpty-3.0.3-cp313-cp313-win_amd64.whl (pywinpty==3.0.3)\n",
      "  \u001b[1m\u001b[31mCaused by\u001b[39m\u001b[0m: failed to create directory `c:\\Program Files\\Python313\\Lib\\site-packages\\pywinpty-3.0.3.dist-info`: Accès refusé. (os error 5)\n"
     ]
    }
   ],
   "source": [
    "!pip install uv\n",
    "!uv pip install --system -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f9d94d-ecc7-45bf-812d-05d62f3b42d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.spatial import cKDTree\n",
    "import pystac_client\n",
    "import planetary_computer as pc\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "print(\"Imports OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74652f9-d132-4bd2-b444-beabc82fbeb0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Étape 2 : Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables de base : ['pet', 'aet', 'ppt', 'tmax', 'tmin', 'soil', 'def', 'pdsi', 'vpd', 'ws']\n",
      "Variables avec agrégations temporelles : ['ppt', 'soil', 'def', 'vpd']\n",
      "Nombre de lags : 3 mois\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "OUTPUT_DIR = \"../data/processed\"\n",
    "\n",
    "# Variables de base à extraire\n",
    "TERRACLIMATE_VARIABLES = ['pet', 'aet', 'ppt', 'tmax', 'tmin', 'soil', 'def', 'pdsi', 'vpd', 'ws']\n",
    "\n",
    "# Variables pour lesquelles on veut des lags et cumuls\n",
    "TEMPORAL_VARS = ['ppt', 'soil', 'def', 'vpd']\n",
    "\n",
    "# Nombre de mois de lag\n",
    "N_LAGS = 3\n",
    "\n",
    "print(f\"Variables de base : {TERRACLIMATE_VARIABLES}\")\n",
    "print(f\"Variables avec agrégations temporelles : {TEMPORAL_VARS}\")\n",
    "print(f\"Nombre de lags : {N_LAGS} mois\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d42366-3ab5-47b4-bf86-32bb9c63eb7a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Étape 3 : Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad324597-e798-4127-8668-da9ab6873467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_terraclimate_dataset():\n",
    "    \"\"\"\n",
    "    Charge le dataset TerraClimate depuis Microsoft Planetary Computer.\n",
    "    \"\"\"\n",
    "    catalog = pystac_client.Client.open(\n",
    "        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "        modifier=pc.sign_inplace,\n",
    "    )\n",
    "    \n",
    "    collection = catalog.get_collection(\"terraclimate\")\n",
    "    asset = collection.assets[\"zarr-abfs\"]\n",
    "\n",
    "    if \"xarray:storage_options\" in asset.extra_fields:\n",
    "        ds = xr.open_zarr(\n",
    "            asset.href,\n",
    "            storage_options=asset.extra_fields[\"xarray:storage_options\"],\n",
    "            consolidated=True,\n",
    "        )\n",
    "    else:\n",
    "        ds = xr.open_dataset(\n",
    "            asset.href,\n",
    "            **asset.extra_fields[\"xarray:open_kwargs\"],\n",
    "        )\n",
    "\n",
    "    print(f\"Dataset chargé ! Variables : {list(ds.data_vars)}\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "filter-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_terraclimate(ds, var, start_date=\"2010-01-01\", end_date=\"2015-12-31\"):\n",
    "    \"\"\"\n",
    "    Filtre le dataset TerraClimate pour une variable et une zone.\n",
    "    Note: On commence en 2010 pour avoir les lags pour les données 2011.\n",
    "    \"\"\"\n",
    "    ds_filtered = ds[var].sel(\n",
    "        time=slice(start_date, end_date),\n",
    "        lat=slice(-21.72, -35.18),\n",
    "        lon=slice(14.97, 32.79)\n",
    "    )\n",
    "    \n",
    "    df = ds_filtered.to_dataframe().reset_index()\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df = df.rename(columns={\"lat\": \"Latitude\", \"lon\": \"Longitude\", \"time\": \"year_month\"})\n",
    "    \n",
    "    # Convertir en période mensuelle\n",
    "    df['year_month'] = df['year_month'].dt.to_period('M')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "assign-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kdtree(climate_df):\n",
    "    \"\"\"\n",
    "    Construit un KD-Tree pour trouver les points climatiques les plus proches.\n",
    "    \"\"\"\n",
    "    unique_coords = climate_df[['Latitude', 'Longitude']].drop_duplicates().reset_index(drop=True)\n",
    "    coords_radians = np.radians(unique_coords.values)\n",
    "    tree = cKDTree(coords_radians)\n",
    "    return tree, unique_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "extract-with-lags",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_with_temporal_features(sites_df, climate_df, var_name, tree, unique_coords, n_lags=3):\n",
    "    \"\"\"\n",
    "    Extrait une variable climatique avec ses agrégations temporelles.\n",
    "    \n",
    "    Pour chaque site et date, extrait :\n",
    "    - Valeur du mois actuel\n",
    "    - Valeurs des n mois précédents (lag1, lag2, lag3)\n",
    "    - Cumul sur n+1 mois\n",
    "    - Anomalie (écart à la moyenne du même mois sur toutes les années)\n",
    "    \"\"\"\n",
    "    sites_df = sites_df.copy().reset_index(drop=True)\n",
    "    \n",
    "    # Trouver les coordonnées climatiques les plus proches\n",
    "    site_coords = np.radians(sites_df[['Latitude', 'Longitude']].values)\n",
    "    _, indices = tree.query(site_coords, k=1)\n",
    "    sites_df['nearest_lat'] = unique_coords.iloc[indices]['Latitude'].values\n",
    "    sites_df['nearest_lon'] = unique_coords.iloc[indices]['Longitude'].values\n",
    "    \n",
    "    # Convertir les dates\n",
    "    sites_df['Sample Date'] = pd.to_datetime(sites_df['Sample Date'], dayfirst=True, errors='coerce')\n",
    "    sites_df['year_month'] = sites_df['Sample Date'].dt.to_period('M')\n",
    "    \n",
    "    # Préparer le DataFrame climatique pour la jointure\n",
    "    climate_pivot = climate_df.pivot_table(\n",
    "        index=['Latitude', 'Longitude'],\n",
    "        columns='year_month',\n",
    "        values=var_name,\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    \n",
    "    # Calculer la moyenne saisonnière (par mois de l'année)\n",
    "    climate_df['month'] = climate_df['year_month'].apply(lambda x: x.month)\n",
    "    seasonal_mean = climate_df.groupby(['Latitude', 'Longitude', 'month'])[var_name].mean().reset_index()\n",
    "    seasonal_mean = seasonal_mean.rename(columns={var_name: f'{var_name}_seasonal_mean'})\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, row in sites_df.iterrows():\n",
    "        lat, lon = row['nearest_lat'], row['nearest_lon']\n",
    "        ym = row['year_month']\n",
    "        month = ym.month\n",
    "        \n",
    "        result = {}\n",
    "        \n",
    "        try:\n",
    "            # Valeur actuelle\n",
    "            current_val = climate_pivot.loc[(lat, lon), ym] if ym in climate_pivot.columns else np.nan\n",
    "            result[var_name] = current_val\n",
    "            \n",
    "            # Lags (mois précédents)\n",
    "            lag_values = [current_val] if not pd.isna(current_val) else []\n",
    "            for lag in range(1, n_lags + 1):\n",
    "                lag_ym = ym - lag\n",
    "                lag_val = climate_pivot.loc[(lat, lon), lag_ym] if lag_ym in climate_pivot.columns else np.nan\n",
    "                result[f'{var_name}_lag{lag}'] = lag_val\n",
    "                if not pd.isna(lag_val):\n",
    "                    lag_values.append(lag_val)\n",
    "            \n",
    "            # Cumul sur n_lags+1 mois\n",
    "            result[f'{var_name}_sum{n_lags+1}'] = np.sum(lag_values) if lag_values else np.nan\n",
    "            \n",
    "            # Moyenne sur n_lags+1 mois\n",
    "            result[f'{var_name}_mean{n_lags+1}'] = np.mean(lag_values) if lag_values else np.nan\n",
    "            \n",
    "            # Anomalie (écart à la moyenne saisonnière)\n",
    "            seasonal = seasonal_mean[(seasonal_mean['Latitude'] == lat) & \n",
    "                                     (seasonal_mean['Longitude'] == lon) & \n",
    "                                     (seasonal_mean['month'] == month)]\n",
    "            if len(seasonal) > 0 and not pd.isna(current_val):\n",
    "                seasonal_val = seasonal[f'{var_name}_seasonal_mean'].values[0]\n",
    "                result[f'{var_name}_anomaly'] = current_val - seasonal_val\n",
    "            else:\n",
    "                result[f'{var_name}_anomaly'] = np.nan\n",
    "                \n",
    "        except (KeyError, IndexError):\n",
    "            result[var_name] = np.nan\n",
    "            for lag in range(1, n_lags + 1):\n",
    "                result[f'{var_name}_lag{lag}'] = np.nan\n",
    "            result[f'{var_name}_sum{n_lags+1}'] = np.nan\n",
    "            result[f'{var_name}_mean{n_lags+1}'] = np.nan\n",
    "            result[f'{var_name}_anomaly'] = np.nan\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extract-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_simple(sites_df, climate_df, var_name, tree, unique_coords):\n",
    "    \"\"\"\n",
    "    Extrait une variable climatique simple (sans agrégations temporelles).\n",
    "    \"\"\"\n",
    "    sites_df = sites_df.copy().reset_index(drop=True)\n",
    "    \n",
    "    # Trouver les coordonnées climatiques les plus proches\n",
    "    site_coords = np.radians(sites_df[['Latitude', 'Longitude']].values)\n",
    "    _, indices = tree.query(site_coords, k=1)\n",
    "    sites_df['nearest_lat'] = unique_coords.iloc[indices]['Latitude'].values\n",
    "    sites_df['nearest_lon'] = unique_coords.iloc[indices]['Longitude'].values\n",
    "    \n",
    "    # Convertir les dates\n",
    "    sites_df['Sample Date'] = pd.to_datetime(sites_df['Sample Date'], dayfirst=True, errors='coerce')\n",
    "    sites_df['year_month'] = sites_df['Sample Date'].dt.to_period('M')\n",
    "    \n",
    "    # Merge\n",
    "    result = sites_df.merge(\n",
    "        climate_df[['Latitude', 'Longitude', 'year_month', var_name]],\n",
    "        left_on=['nearest_lat', 'nearest_lon', 'year_month'],\n",
    "        right_on=['Latitude', 'Longitude', 'year_month'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return pd.DataFrame({var_name: result[var_name].values})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8065c081-51ca-439e-b42d-fa1afcb961e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Étape 4 : Extraction pour les données d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54291642-9b6a-42a5-8370-12c230b755d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sites training : 9319\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Sample Date</th>\n",
       "      <th>Total Alkalinity</th>\n",
       "      <th>Electrical Conductance</th>\n",
       "      <th>Dissolved Reactive Phosphorus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-28.760833</td>\n",
       "      <td>17.730278</td>\n",
       "      <td>02-01-2011</td>\n",
       "      <td>128.912</td>\n",
       "      <td>555.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-26.861111</td>\n",
       "      <td>28.884722</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>74.720</td>\n",
       "      <td>162.9</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-26.450000</td>\n",
       "      <td>28.085833</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>89.254</td>\n",
       "      <td>573.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-27.671111</td>\n",
       "      <td>27.236944</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>82.000</td>\n",
       "      <td>203.6</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-27.356667</td>\n",
       "      <td>27.286389</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>56.100</td>\n",
       "      <td>145.1</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Latitude  Longitude Sample Date  Total Alkalinity  Electrical Conductance  \\\n",
       "0 -28.760833  17.730278  02-01-2011           128.912                   555.0   \n",
       "1 -26.861111  28.884722  03-01-2011            74.720                   162.9   \n",
       "2 -26.450000  28.085833  03-01-2011            89.254                   573.0   \n",
       "3 -27.671111  27.236944  03-01-2011            82.000                   203.6   \n",
       "4 -27.356667  27.286389  03-01-2011            56.100                   145.1   \n",
       "\n",
       "   Dissolved Reactive Phosphorus  \n",
       "0                           10.0  \n",
       "1                          163.0  \n",
       "2                           80.0  \n",
       "3                          101.0  \n",
       "4                          151.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Charger les données de qualité d'eau\n",
    "Water_Quality_df = pd.read_csv(\"../data/raw/water_quality_training_dataset.csv\")\n",
    "print(f\"Sites training : {len(Water_Quality_df)}\")\n",
    "display(Water_Quality_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34229a57-1579-4615-8b07-64fde8299e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Connexion à Microsoft Planetary Computer...\n",
      "Dataset chargé ! Variables : ['aet', 'def', 'pdsi', 'pet', 'ppt', 'q', 'soil', 'srad', 'swe', 'tmax', 'tmin', 'vap', 'vpd', 'ws']\n",
      "\n",
      "2. Extraction des variables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Variables: 100%|██████████| 10/10 [21:31<00:00, 129.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extraction terminée : 9319 lignes, 37 colonnes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXTRACTION TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"1. Connexion à Microsoft Planetary Computer...\")\n",
    "ds = load_terraclimate_dataset()\n",
    "\n",
    "# Initialiser le DataFrame final\n",
    "Training_df = Water_Quality_df[['Latitude', 'Longitude', 'Sample Date']].copy()\n",
    "\n",
    "# Cache pour réutilisation avec validation\n",
    "tc_cache = {}\n",
    "\n",
    "print(\"\\n2. Extraction des variables...\")\n",
    "\n",
    "for var in tqdm(TERRACLIMATE_VARIABLES, desc=\"Variables\"):\n",
    "    # Filtrer les données (on prend depuis 2010 pour avoir les lags)\n",
    "    climate_df = filter_terraclimate(ds, var, start_date=\"2010-01-01\", end_date=\"2015-12-31\")\n",
    "    tc_cache[var] = climate_df\n",
    "    \n",
    "    # Construire le KD-Tree\n",
    "    tree, unique_coords = build_kdtree(climate_df)\n",
    "    \n",
    "    if var in TEMPORAL_VARS:\n",
    "        # Extraction avec agrégations temporelles\n",
    "        var_df = extract_with_temporal_features(\n",
    "            Water_Quality_df, climate_df, var, tree, unique_coords, n_lags=N_LAGS\n",
    "        )\n",
    "    else:\n",
    "        # Extraction simple\n",
    "        var_df = extract_simple(Water_Quality_df, climate_df, var, tree, unique_coords)\n",
    "    \n",
    "    # Ajouter au DataFrame final\n",
    "    for col in var_df.columns:\n",
    "        Training_df[col] = var_df[col].values\n",
    "\n",
    "print(f\"\\nExtraction terminée : {len(Training_df)} lignes, {len(Training_df.columns)} colonnes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c2eebd3-78ab-4306-ba51-57542dee97bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier créé : ../data/processed\\terraclimate_features_training_v2.csv\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder\n",
    "output_path = os.path.join(OUTPUT_DIR, 'terraclimate_features_training_v2.csv')\n",
    "Training_df.to_csv(output_path, index=False)\n",
    "print(f\"Fichier créé : {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c2114e6-8204-4cf7-8f0d-16745cb1fd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes créées (37) :\n",
      "['Latitude', 'Longitude', 'Sample Date', 'pet', 'aet', 'ppt', 'ppt_lag1', 'ppt_lag2', 'ppt_lag3', 'ppt_sum4', 'ppt_mean4', 'ppt_anomaly', 'tmax', 'tmin', 'soil', 'soil_lag1', 'soil_lag2', 'soil_lag3', 'soil_sum4', 'soil_mean4', 'soil_anomaly', 'def', 'def_lag1', 'def_lag2', 'def_lag3', 'def_sum4', 'def_mean4', 'def_anomaly', 'pdsi', 'vpd', 'vpd_lag1', 'vpd_lag2', 'vpd_lag3', 'vpd_sum4', 'vpd_mean4', 'vpd_anomaly', 'ws']\n",
      "\n",
      "Aperçu des nouvelles features temporelles (ppt) :\n",
      "           ppt  ppt_lag1  ppt_lag2  ppt_lag3  ppt_sum4  ppt_mean4  ppt_anomaly\n",
      "count  9319.00   9319.00   9319.00   9319.00   9319.00    9319.00      9319.00\n",
      "mean     41.89     43.81     45.40     47.38    178.49      44.62        -1.86\n",
      "std      44.91     47.23     49.10     49.59    137.17      34.29        30.61\n",
      "min       0.00      0.00      0.00      0.00      0.00       0.00      -148.07\n",
      "25%       7.30      7.00      7.10      8.50     70.90      17.73       -17.89\n",
      "50%      28.40     30.10     31.00     33.60    144.80      36.20        -4.27\n",
      "75%      59.90     62.30     65.55     68.40    269.25      67.31         9.82\n",
      "max     374.50    407.00    407.00    374.50    992.30     248.08       248.02\n"
     ]
    }
   ],
   "source": [
    "# Aperçu\n",
    "print(f\"Colonnes créées ({len(Training_df.columns)}) :\")\n",
    "print(list(Training_df.columns))\n",
    "\n",
    "print(f\"\\nAperçu des nouvelles features temporelles (ppt) :\")\n",
    "ppt_cols = [c for c in Training_df.columns if c.startswith('ppt')]\n",
    "print(Training_df[ppt_cols].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91cf806-2228-4b46-a0c1-8b1afb01bc6d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Étape 5 : Extraction pour les données de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ed3f63d-3a89-4145-b2a6-917c2e98fa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sites validation : 200\n"
     ]
    }
   ],
   "source": [
    "# Charger le template de soumission\n",
    "Validation_df = pd.read_csv('../data/raw/submission_template.csv')\n",
    "print(f\"Sites validation : {len(Validation_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4a6c090-39df-4923-b03b-1fc257e2952b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction des variables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Variables: 100%|██████████| 10/10 [08:01<00:00, 48.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extraction terminée : 200 lignes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXTRACTION VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "# Initialiser le DataFrame final\n",
    "Validation_result_df = Validation_df[['Latitude', 'Longitude', 'Sample Date']].copy()\n",
    "\n",
    "print(\"Extraction des variables...\")\n",
    "\n",
    "for var in tqdm(TERRACLIMATE_VARIABLES, desc=\"Variables\"):\n",
    "    # Réutiliser les données du cache\n",
    "    climate_df = tc_cache[var]\n",
    "    \n",
    "    # Construire le KD-Tree\n",
    "    tree, unique_coords = build_kdtree(climate_df)\n",
    "    \n",
    "    if var in TEMPORAL_VARS:\n",
    "        var_df = extract_with_temporal_features(\n",
    "            Validation_df, climate_df, var, tree, unique_coords, n_lags=N_LAGS\n",
    "        )\n",
    "    else:\n",
    "        var_df = extract_simple(Validation_df, climate_df, var, tree, unique_coords)\n",
    "    \n",
    "    for col in var_df.columns:\n",
    "        Validation_result_df[col] = var_df[col].values\n",
    "\n",
    "print(f\"\\nExtraction terminée : {len(Validation_result_df)} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f0fc36a-b4b7-44c4-bc59-7a1f6216d5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier créé : ../data/processed\\terraclimate_features_validation_v2.csv\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder\n",
    "output_path = os.path.join(OUTPUT_DIR, 'terraclimate_features_validation_v2.csv')\n",
    "Validation_result_df.to_csv(output_path, index=False)\n",
    "print(f\"Fichier créé : {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b2806-b54b-4182-bf85-680f13edf181",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Résumé des features créées\n",
    "\n",
    "### Variables de base (10)\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| pet | Évapotranspiration potentielle |\n",
    "| aet | Évapotranspiration réelle |\n",
    "| ppt | Précipitations |\n",
    "| tmax | Température max |\n",
    "| tmin | Température min |\n",
    "| soil | Humidité du sol |\n",
    "| def | Déficit hydrique |\n",
    "| pdsi | Indice de sécheresse |\n",
    "| vpd | Déficit de pression de vapeur |\n",
    "| ws | Vitesse du vent |\n",
    "\n",
    "### Agrégations temporelles (pour ppt, soil, def, vpd)\n",
    "| Suffixe | Description | Exemple |\n",
    "|---------|-------------|--------|\n",
    "| `_lag1` | Valeur mois-1 | ppt_lag1 |\n",
    "| `_lag2` | Valeur mois-2 | ppt_lag2 |\n",
    "| `_lag3` | Valeur mois-3 | ppt_lag3 |\n",
    "| `_sum4` | Cumul 4 derniers mois | ppt_sum4 |\n",
    "| `_mean4` | Moyenne 4 derniers mois | ppt_mean4 |\n",
    "| `_anomaly` | Écart à la moyenne saisonnière | ppt_anomaly |\n",
    "\n",
    "### Total de features\n",
    "- Variables simples : 6 (pet, aet, tmax, tmin, pdsi, ws)\n",
    "- Variables avec temporel : 4 × 7 = 28 (ppt, soil, def, vpd avec 6 features chacune + la valeur de base)\n",
    "- **Total : 34 features climatiques**\n",
    "\n",
    "### Fichiers créés\n",
    "| Fichier | Description |\n",
    "|---------|-------------|\n",
    "| terraclimate_features_training_v2.csv | Training avec agrégations temporelles |\n",
    "| terraclimate_features_validation_v2.csv | Validation avec agrégations temporelles |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
