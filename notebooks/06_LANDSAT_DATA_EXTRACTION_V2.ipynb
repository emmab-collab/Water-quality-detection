{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Landsat V2 - Extraction avec Buffer et Statistiques\n",
    "\n",
    "## Améliorations par rapport à V1\n",
    "\n",
    "**V1** : Extrait un **pixel unique** au point de mesure\n",
    "\n",
    "**V2** : Extrait un **buffer de 200m** autour du point avec **statistiques** :\n",
    "\n",
    "| Problème V1 | Solution V2 |\n",
    "|-------------|-------------|\n",
    "| Un pixel = bruit de mesure | Buffer = valeur plus stable |\n",
    "| Pas d'info sur l'hétérogénéité | Écart-type = variabilité locale |\n",
    "| Sensible aux erreurs GPS | Buffer = plus robuste |\n",
    "\n",
    "## Nouvelles features\n",
    "\n",
    "Pour chaque bande/indice, on calcule :\n",
    "\n",
    "| Suffixe | Description | Utilité |\n",
    "|---------|-------------|--------|\n",
    "| (rien) | Moyenne dans le buffer | Valeur représentative |\n",
    "| `_std` | Écart-type dans le buffer | Hétérogénéité spatiale |\n",
    "\n",
    "**Exemple** : Pour NDVI, on aura `NDVI` (moyenne) et `NDVI_std` (variabilité)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-install",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install-deps",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q odc-stac planetary-computer pystac-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pystac_client\n",
    "import planetary_computer as pc\n",
    "from odc.stac import stac_load\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "print(\"Imports OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Buffer: 0.002 degrés (~222m)\n",
      "  Seuil nuages: 20%\n",
      "  Workers: 6\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Bandes spectrales à extraire\n",
    "ALL_BANDS = [\"blue\", \"green\", \"red\", \"nir08\", \"swir16\", \"swir22\"]\n",
    "\n",
    "# Taille du buffer en degrés (~200m à cette latitude)\n",
    "# 1 degré ≈ 111 km, donc 0.002 ≈ 220m\n",
    "BUFFER_SIZE = 0.002\n",
    "\n",
    "# Période de recherche\n",
    "DATE_RANGE = \"2011-01-01/2015-12-31\"\n",
    "\n",
    "# Seuil de couverture nuageuse\n",
    "MAX_CLOUD_COVER = 20\n",
    "\n",
    "# Nombre de workers parallèles\n",
    "N_WORKERS = 6\n",
    "\n",
    "# Chemins\n",
    "WATER_QUALITY_FILE = \"../data/raw/water_quality_training_dataset.csv\"\n",
    "SUBMISSION_FILE = \"../data/raw/submission_template.csv\"\n",
    "OUTPUT_DIR = \"../data/processed\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Buffer: {BUFFER_SIZE} degrés (~{BUFFER_SIZE * 111000:.0f}m)\")\n",
    "print(f\"  Seuil nuages: {MAX_CLOUD_COVER}%\")\n",
    "print(f\"  Workers: {N_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "func-title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fonctions d'extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "func-indices",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_indices(data):\n",
    "    \"\"\"\n",
    "    Calcule les indices spectraux à partir des bandes.\n",
    "    \n",
    "    Paramètres:\n",
    "        data : xarray.Dataset avec les bandes\n",
    "    \n",
    "    Retourne:\n",
    "        dict avec les indices calculés (arrays 2D)\n",
    "    \"\"\"\n",
    "    eps = 1e-10\n",
    "    indices = {}\n",
    "    \n",
    "    nir = data['nir08'].values.astype(float)\n",
    "    green = data['green'].values.astype(float)\n",
    "    red = data['red'].values.astype(float)\n",
    "    swir16 = data['swir16'].values.astype(float)\n",
    "    \n",
    "    # NDVI = (NIR - Red) / (NIR + Red)\n",
    "    indices['NDVI'] = (nir - red) / (nir + red + eps)\n",
    "    \n",
    "    # NDWI = (Green - NIR) / (Green + NIR)\n",
    "    indices['NDWI'] = (green - nir) / (green + nir + eps)\n",
    "    \n",
    "    # NDMI = (NIR - SWIR16) / (NIR + SWIR16)\n",
    "    indices['NDMI'] = (nir - swir16) / (nir + swir16 + eps)\n",
    "    \n",
    "    # MNDWI = (Green - SWIR16) / (Green + SWIR16)\n",
    "    indices['MNDWI'] = (green - swir16) / (green + swir16 + eps)\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "func-extract",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonction extract_landsat_with_buffer() définie!\n"
     ]
    }
   ],
   "source": [
    "def extract_landsat_with_buffer(df, buffer_size=BUFFER_SIZE, max_cloud_cover=MAX_CLOUD_COVER, \n",
    "                                 n_workers=N_WORKERS, save_every=50, \n",
    "                                 backup_path=\"../data/processed/landsat_buffer_backup.csv\"):\n",
    "    \"\"\"\n",
    "    Extrait les valeurs Landsat avec buffer et statistiques.\n",
    "    \n",
    "    Pour chaque point :\n",
    "    1. Crée un buffer carré autour du point\n",
    "    2. Charge les pixels Landsat dans ce buffer\n",
    "    3. Calcule moyenne et écart-type pour chaque bande/indice\n",
    "    \n",
    "    Retourne:\n",
    "        DataFrame avec colonnes: bande, bande_std, indice, indice_std\n",
    "    \"\"\"\n",
    "    df = df.copy().reset_index(drop=True)\n",
    "    \n",
    "    # Liste des features à extraire\n",
    "    bands = ['blue', 'green', 'red', 'nir', 'swir16', 'swir22']\n",
    "    indices = ['NDVI', 'NDWI', 'NDMI', 'MNDWI']\n",
    "    \n",
    "    # Colonnes de résultats (moyenne + std)\n",
    "    result_cols = bands + [f'{b}_std' for b in bands] + indices + [f'{i}_std' for i in indices]\n",
    "    results = {col: np.full(len(df), np.nan) for col in result_cols}\n",
    "    \n",
    "    completed_count = 0\n",
    "    \n",
    "    print(f\"Extraction avec buffer pour {len(df)} points\")\n",
    "    print(f\"  Buffer: ~{buffer_size * 111000:.0f}m | Nuages max: {max_cloud_cover}%\")\n",
    "    print(f\"  Sauvegarde tous les {save_every} points\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    def extract_point(args):\n",
    "        \"\"\"Extrait les données pour un point avec buffer.\"\"\"\n",
    "        idx, lat, lon, sample_date = args\n",
    "        \n",
    "        try:\n",
    "            if pd.isna(sample_date):\n",
    "                return idx, {col: np.nan for col in result_cols}\n",
    "            \n",
    "            if isinstance(sample_date, str):\n",
    "                sample_date = pd.to_datetime(sample_date, dayfirst=True, errors='coerce')\n",
    "            if pd.isna(sample_date):\n",
    "                return idx, {col: np.nan for col in result_cols}\n",
    "            \n",
    "            # Bounding box avec buffer\n",
    "            bbox = [\n",
    "                lon - buffer_size,\n",
    "                lat - buffer_size,\n",
    "                lon + buffer_size,\n",
    "                lat + buffer_size\n",
    "            ]\n",
    "            \n",
    "            # Connexion au catalogue\n",
    "            catalog = pystac_client.Client.open(\n",
    "                \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "                modifier=pc.sign_inplace,\n",
    "            )\n",
    "            \n",
    "            # Recherche des scènes\n",
    "            search = catalog.search(\n",
    "                collections=[\"landsat-c2-l2\"],\n",
    "                bbox=bbox,\n",
    "                datetime=DATE_RANGE,\n",
    "                query={\"eo:cloud_cover\": {\"lt\": max_cloud_cover}},\n",
    "            )\n",
    "            items = list(search.item_collection())\n",
    "            \n",
    "            if not items:\n",
    "                return idx, {col: np.nan for col in result_cols}\n",
    "            \n",
    "            # Sélectionner la scène la plus proche de la date\n",
    "            sample_date_utc = sample_date.tz_localize(\"UTC\") if sample_date.tzinfo is None else sample_date\n",
    "            best_item = min(items, key=lambda x: abs(\n",
    "                pd.to_datetime(x.properties[\"datetime\"]).tz_convert(\"UTC\") - sample_date_utc\n",
    "            ))\n",
    "            \n",
    "            # Charger les données\n",
    "            signed_item = pc.sign(best_item)\n",
    "            data = stac_load([signed_item], bands=ALL_BANDS, bbox=bbox).isel(time=0)\n",
    "            \n",
    "            result = {}\n",
    "            \n",
    "            # Statistiques pour chaque bande\n",
    "            band_mapping = {'nir08': 'nir'}  # Renommer nir08 -> nir\n",
    "            \n",
    "            for band in ALL_BANDS:\n",
    "                band_data = data[band].values.astype(float)\n",
    "                band_data = band_data[band_data > 0]  # Exclure les valeurs 0 (nodata)\n",
    "                \n",
    "                output_name = band_mapping.get(band, band)\n",
    "                \n",
    "                if len(band_data) > 0:\n",
    "                    result[output_name] = float(np.nanmean(band_data))\n",
    "                    result[f'{output_name}_std'] = float(np.nanstd(band_data))\n",
    "                else:\n",
    "                    result[output_name] = np.nan\n",
    "                    result[f'{output_name}_std'] = np.nan\n",
    "            \n",
    "            # Calculer les indices et leurs stats\n",
    "            computed_indices = compute_indices(data)\n",
    "            \n",
    "            for idx_name, idx_data in computed_indices.items():\n",
    "                idx_flat = idx_data.flatten()\n",
    "                idx_flat = idx_flat[~np.isnan(idx_flat)]  # Exclure NaN\n",
    "                idx_flat = idx_flat[(idx_flat >= -1) & (idx_flat <= 1)]  # Valeurs valides\n",
    "                \n",
    "                if len(idx_flat) > 0:\n",
    "                    result[idx_name] = float(np.nanmean(idx_flat))\n",
    "                    result[f'{idx_name}_std'] = float(np.nanstd(idx_flat))\n",
    "                else:\n",
    "                    result[idx_name] = np.nan\n",
    "                    result[f'{idx_name}_std'] = np.nan\n",
    "            \n",
    "            return idx, result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return idx, {col: np.nan for col in result_cols}\n",
    "    \n",
    "    # Préparer les arguments\n",
    "    args_list = [\n",
    "        (idx, df.loc[idx, 'Latitude'], df.loc[idx, 'Longitude'], df.loc[idx, 'Sample Date'])\n",
    "        for idx in df.index\n",
    "    ]\n",
    "    \n",
    "    # Extraction parallèle\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        futures = {executor.submit(extract_point, args): args[0] for args in args_list}\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Extraction\"):\n",
    "            try:\n",
    "                idx, point_results = future.result(timeout=120)\n",
    "                for col in result_cols:\n",
    "                    if col in point_results:\n",
    "                        results[col][idx] = point_results[col]\n",
    "                \n",
    "                completed_count += 1\n",
    "                \n",
    "                # Sauvegarde incrémentale\n",
    "                if completed_count % save_every == 0:\n",
    "                    backup_df = pd.DataFrame(results)\n",
    "                    backup_df.to_csv(backup_path, index=False)\n",
    "                    \n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    # Sauvegarde finale\n",
    "    final_df = pd.DataFrame(results)\n",
    "    final_df.to_csv(backup_path, index=False)\n",
    "    print(f\"\\nExtraction terminée! Sauvegarde: {backup_path}\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "print(\"Fonction extract_landsat_with_buffer() définie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extraction pour les données d'entraînement\n",
    "\n",
    "⏱️ **Temps estimé** : ~3-4 heures pour ~9300 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "load-training",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sites training : 9319\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Sample Date</th>\n",
       "      <th>Total Alkalinity</th>\n",
       "      <th>Electrical Conductance</th>\n",
       "      <th>Dissolved Reactive Phosphorus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-28.760833</td>\n",
       "      <td>17.730278</td>\n",
       "      <td>02-01-2011</td>\n",
       "      <td>128.912</td>\n",
       "      <td>555.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-26.861111</td>\n",
       "      <td>28.884722</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>74.720</td>\n",
       "      <td>162.9</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-26.450000</td>\n",
       "      <td>28.085833</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>89.254</td>\n",
       "      <td>573.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-27.671111</td>\n",
       "      <td>27.236944</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>82.000</td>\n",
       "      <td>203.6</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-27.356667</td>\n",
       "      <td>27.286389</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>56.100</td>\n",
       "      <td>145.1</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Latitude  Longitude Sample Date  Total Alkalinity  Electrical Conductance  \\\n",
       "0 -28.760833  17.730278  02-01-2011           128.912                   555.0   \n",
       "1 -26.861111  28.884722  03-01-2011            74.720                   162.9   \n",
       "2 -26.450000  28.085833  03-01-2011            89.254                   573.0   \n",
       "3 -27.671111  27.236944  03-01-2011            82.000                   203.6   \n",
       "4 -27.356667  27.286389  03-01-2011            56.100                   145.1   \n",
       "\n",
       "   Dissolved Reactive Phosphorus  \n",
       "0                           10.0  \n",
       "1                          163.0  \n",
       "2                           80.0  \n",
       "3                          101.0  \n",
       "4                          151.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Charger les données de qualité d'eau\n",
    "water_quality = pd.read_csv(WATER_QUALITY_FILE)\n",
    "print(f\"Sites training : {len(water_quality)}\")\n",
    "display(water_quality.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "extract-training",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction avec buffer pour 9319 points\n",
      "  Buffer: ~222m | Nuages max: 20%\n",
      "  Sauvegarde tous les 50 points\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction: 100%|██████████| 9319/9319 [3:27:37<00:00,  1.34s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extraction terminée! Sauvegarde: ../data/processed/landsat_training_buffer_backup.csv\n",
      "\n",
      "Features extraites : 20 colonnes\n",
      "['blue', 'green', 'red', 'nir', 'swir16', 'swir22', 'blue_std', 'green_std', 'red_std', 'nir_std', 'swir16_std', 'swir22_std', 'NDVI', 'NDWI', 'NDMI', 'MNDWI', 'NDVI_std', 'NDWI_std', 'NDMI_std', 'MNDWI_std']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXTRACTION TRAINING AVEC BUFFER\n",
    "# =============================================================================\n",
    "\n",
    "training_features = extract_landsat_with_buffer(\n",
    "    water_quality,\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    max_cloud_cover=MAX_CLOUD_COVER,\n",
    "    n_workers=N_WORKERS,\n",
    "    save_every=50,\n",
    "    backup_path=\"../data/processed/landsat_training_buffer_backup.csv\"\n",
    ")\n",
    "\n",
    "print(f\"\\nFeatures extraites : {len(training_features.columns)} colonnes\")\n",
    "print(training_features.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "save-training",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier sauvegardé : ../data/processed\\landsat_features_training_v2.csv\n",
      "  - 9319 lignes\n",
      "  - 23 colonnes\n",
      "\n",
      "Taux de complétion :\n",
      "  blue: 98.9%\n",
      "  nir: 98.9%\n",
      "  NDVI: 99.1%\n",
      "  NDVI_std: 99.1%\n"
     ]
    }
   ],
   "source": [
    "# Créer le DataFrame final avec coordonnées\n",
    "training_df = pd.DataFrame({\n",
    "    'Latitude': water_quality['Latitude'].values,\n",
    "    'Longitude': water_quality['Longitude'].values,\n",
    "    'Sample Date': water_quality['Sample Date'].values,\n",
    "})\n",
    "\n",
    "# Ajouter les features Landsat\n",
    "for col in training_features.columns:\n",
    "    training_df[col] = training_features[col].values\n",
    "\n",
    "# Sauvegarder\n",
    "output_path = os.path.join(OUTPUT_DIR, 'landsat_features_training_v2.csv')\n",
    "training_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Fichier sauvegardé : {output_path}\")\n",
    "print(f\"  - {len(training_df)} lignes\")\n",
    "print(f\"  - {len(training_df.columns)} colonnes\")\n",
    "\n",
    "# Statistiques\n",
    "print(f\"\\nTaux de complétion :\")\n",
    "for col in ['blue', 'nir', 'NDVI', 'NDVI_std']:\n",
    "    if col in training_df.columns:\n",
    "        taux = (1 - training_df[col].isna().mean()) * 100\n",
    "        print(f\"  {col}: {taux:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extraction pour les données de validation\n",
    "\n",
    "⏱️ **Temps estimé** : ~15-20 minutes pour ~200 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "load-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sites validation : 200\n"
     ]
    }
   ],
   "source": [
    "# Charger le template de submission\n",
    "submission = pd.read_csv(SUBMISSION_FILE)\n",
    "print(f\"Sites validation : {len(submission)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "extract-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction avec buffer pour 200 points\n",
      "  Buffer: ~222m | Nuages max: 20%\n",
      "  Sauvegarde tous les 20 points\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction: 100%|██████████| 200/200 [04:11<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extraction terminée! Sauvegarde: ../data/processed/landsat_validation_buffer_backup.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXTRACTION VALIDATION AVEC BUFFER\n",
    "# =============================================================================\n",
    "\n",
    "validation_features = extract_landsat_with_buffer(\n",
    "    submission,\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    max_cloud_cover=MAX_CLOUD_COVER,\n",
    "    n_workers=N_WORKERS,\n",
    "    save_every=20,\n",
    "    backup_path=\"../data/processed/landsat_validation_buffer_backup.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "save-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier sauvegardé : ../data/processed\\landsat_features_validation_v2.csv\n",
      "  - 200 lignes\n",
      "  - 23 colonnes\n"
     ]
    }
   ],
   "source": [
    "# Créer le DataFrame final\n",
    "validation_df = pd.DataFrame({\n",
    "    'Latitude': submission['Latitude'].values,\n",
    "    'Longitude': submission['Longitude'].values,\n",
    "    'Sample Date': submission['Sample Date'].values,\n",
    "})\n",
    "\n",
    "for col in validation_features.columns:\n",
    "    validation_df[col] = validation_features[col].values\n",
    "\n",
    "# Sauvegarder\n",
    "output_path = os.path.join(OUTPUT_DIR, 'landsat_features_validation_v2.csv')\n",
    "validation_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Fichier sauvegardé : {output_path}\")\n",
    "print(f\"  - {len(validation_df)} lignes\")\n",
    "print(f\"  - {len(validation_df.columns)} colonnes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Résumé des features créées\n",
    "\n",
    "### Bandes spectrales (12 features)\n",
    "\n",
    "| Bande | Moyenne | Écart-type |\n",
    "|-------|---------|------------|\n",
    "| blue | `blue` | `blue_std` |\n",
    "| green | `green` | `green_std` |\n",
    "| red | `red` | `red_std` |\n",
    "| nir | `nir` | `nir_std` |\n",
    "| swir16 | `swir16` | `swir16_std` |\n",
    "| swir22 | `swir22` | `swir22_std` |\n",
    "\n",
    "### Indices spectraux (8 features)\n",
    "\n",
    "| Indice | Moyenne | Écart-type | Interprétation std |\n",
    "|--------|---------|------------|--------------------|\n",
    "| NDVI | `NDVI` | `NDVI_std` | Hétérogénéité végétation |\n",
    "| NDWI | `NDWI` | `NDWI_std` | Variation présence eau |\n",
    "| NDMI | `NDMI` | `NDMI_std` | Variation humidité |\n",
    "| MNDWI | `MNDWI` | `MNDWI_std` | Variation eau libre |\n",
    "\n",
    "### Total : 20 features Landsat\n",
    "\n",
    "### Fichiers créés\n",
    "\n",
    "| Fichier | Description |\n",
    "|---------|-------------|\n",
    "| landsat_features_training_v2.csv | Training avec buffer + stats |\n",
    "| landsat_features_validation_v2.csv | Validation avec buffer + stats |\n",
    "\n",
    "### Pourquoi l'écart-type est utile ?\n",
    "\n",
    "- **std élevé** = zone hétérogène (mélange eau/terre, bord de rivière)\n",
    "- **std faible** = zone homogène (grand plan d'eau, forêt dense)\n",
    "- Peut aider à distinguer rivière (std élevé) vs lac (std faible)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
