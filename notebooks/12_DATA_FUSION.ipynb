{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": [
    "# Fusion des Données\n",
    "\n",
    "Ce notebook fusionne toutes les sources de données extraites en un seul dataset prêt pour la modélisation.\n",
    "\n",
    "## Sources de données\n",
    "\n",
    "| Source | Features | Description |\n",
    "|--------|----------|-------------|\n",
    "| **Landsat V2** | ~20 | Bandes spectrales + indices (mean, std) |\n",
    "| **TerraClimate V2** | ~34 | Climat avec lags et cumuls temporels |\n",
    "| **ESA WorldCover** | ~11 | % occupation du sol |\n",
    "| **SoilGrids** | 6 | Propriétés du sol (pH, argile, etc.) |\n",
    "| **DEM** | 3 | Altitude, pente, orientation |\n",
    "| **Water Type** | 1 | Type de milieu (rivière/lac) |\n",
    "\n",
    "## Résultat attendu\n",
    "\n",
    "- `merged_training.csv` : Dataset complet pour l'entraînement\n",
    "- `merged_validation.csv` : Dataset complet pour la soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"Imports OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Étape 1 : Charger toutes les sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-raw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Training : (9319, 6)\n",
      "Colonnes : ['Latitude', 'Longitude', 'Sample Date', 'Total Alkalinity', 'Electrical Conductance', 'Dissolved Reactive Phosphorus']\n",
      "\n",
      "Raw Validation : (200, 6)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DONNÉES BRUTES (avec variables cibles)\n",
    "# =============================================================================\n",
    "\n",
    "# Training : contient les variables cibles\n",
    "raw_training = pd.read_csv(\"../data/raw/water_quality_training_dataset.csv\")\n",
    "print(f\"Raw Training : {raw_training.shape}\")\n",
    "print(f\"Colonnes : {list(raw_training.columns)}\")\n",
    "\n",
    "# Validation : template de soumission\n",
    "raw_validation = pd.read_csv(\"../data/raw/submission_template.csv\")\n",
    "print(f\"\\nRaw Validation : {raw_validation.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des fichiers...\n",
      "\n",
      "✅ landsat training : (9319, 23)\n",
      "✅ landsat validation : (200, 23)\n",
      "\n",
      "✅ terraclimate training : (9319, 37)\n",
      "✅ terraclimate validation : (200, 37)\n",
      "\n",
      "✅ worldcover training : (9319, 14)\n",
      "✅ worldcover validation : (200, 14)\n",
      "\n",
      "✅ soilgrids training : (9319, 9)\n",
      "✅ soilgrids validation : (200, 9)\n",
      "\n",
      "✅ dem training : (9319, 6)\n",
      "✅ dem validation : (200, 6)\n",
      "\n",
      "✅ water_type training : (9319, 5)\n",
      "✅ water_type validation : (200, 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CHARGER TOUTES LES FEATURES EXTRAITES\n",
    "# =============================================================================\n",
    "\n",
    "DATA_DIR = \"../data/processed\"\n",
    "\n",
    "# Dictionnaire pour stocker les DataFrames\n",
    "training_dfs = {}\n",
    "validation_dfs = {}\n",
    "\n",
    "# Liste des fichiers à charger\n",
    "sources = {\n",
    "    'landsat': ('landsat_features_training_v2.csv', 'landsat_features_validation_v2.csv'),\n",
    "    'terraclimate': ('terraclimate_features_training_v2.csv', 'terraclimate_features_validation_v2.csv'),\n",
    "    'worldcover': ('worldcover_features_training.csv', 'worldcover_features_validation.csv'),\n",
    "    'soilgrids': ('soilgrids_features_training.csv', 'soilgrids_features_validation.csv'),\n",
    "    'dem': ('dem_features_training.csv', 'dem_features_validation.csv'),\n",
    "    'water_type': ('water_type_training.csv', 'water_type_validation.csv'),\n",
    "}\n",
    "\n",
    "print(\"Chargement des fichiers...\\n\")\n",
    "\n",
    "for name, (train_file, val_file) in sources.items():\n",
    "    train_path = os.path.join(DATA_DIR, train_file)\n",
    "    val_path = os.path.join(DATA_DIR, val_file)\n",
    "    \n",
    "    if os.path.exists(train_path):\n",
    "        training_dfs[name] = pd.read_csv(train_path)\n",
    "        print(f\"✅ {name} training : {training_dfs[name].shape}\")\n",
    "    else:\n",
    "        print(f\"❌ {name} training : fichier non trouvé\")\n",
    "    \n",
    "    if os.path.exists(val_path):\n",
    "        validation_dfs[name] = pd.read_csv(val_path)\n",
    "        print(f\"✅ {name} validation : {validation_dfs[name].shape}\")\n",
    "    else:\n",
    "        print(f\"❌ {name} validation : fichier non trouvé\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect-md",
   "metadata": {},
   "source": [
    "### Inspecter les colonnes de chaque source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "inspect-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LANDSAT (23 colonnes):\n",
      "  Features: ['blue', 'green', 'red', 'nir', 'swir16', 'swir22', 'blue_std', 'green_std', 'red_std', 'nir_std']...\n",
      "\n",
      "TERRACLIMATE (37 colonnes):\n",
      "  Features: ['pet', 'aet', 'ppt', 'ppt_lag1', 'ppt_lag2', 'ppt_lag3', 'ppt_sum4', 'ppt_mean4', 'ppt_anomaly', 'tmax']...\n",
      "\n",
      "WORLDCOVER (14 colonnes):\n",
      "  Features: ['lc_tree', 'lc_shrubland', 'lc_grassland', 'lc_cropland', 'lc_builtup', 'lc_bare', 'lc_snow', 'lc_water', 'lc_wetland', 'lc_mangroves']...\n",
      "\n",
      "SOILGRIDS (9 colonnes):\n",
      "  Features: ['soil_ph', 'soil_clay', 'soil_sand', 'soil_soc', 'soil_cec', 'soil_nitrogen']\n",
      "\n",
      "DEM (6 colonnes):\n",
      "  Features: ['elevation', 'slope', 'aspect']\n",
      "\n",
      "WATER_TYPE (5 colonnes):\n",
      "  Features: ['water_type', 'distance_to_river']\n"
     ]
    }
   ],
   "source": [
    "# Afficher les colonnes de chaque source\n",
    "for name, df in training_dfs.items():\n",
    "    print(f\"\\n{name.upper()} ({len(df.columns)} colonnes):\")\n",
    "    # Exclure les colonnes de jointure\n",
    "    feature_cols = [c for c in df.columns if c not in ['Latitude', 'Longitude', 'Sample Date']]\n",
    "    print(f\"  Features: {feature_cols[:10]}{'...' if len(feature_cols) > 10 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "merge-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Étape 2 : Fusionner les données\n",
    "\n",
    "On fusionne sur les colonnes `Latitude`, `Longitude` et `Sample Date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "merge-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_sources(base_df, source_dfs, merge_keys=['Latitude', 'Longitude', 'Sample Date']):\n",
    "    \"\"\"\n",
    "    Fusionne toutes les sources de données sur les clés spécifiées.\n",
    "    \n",
    "    Paramètres:\n",
    "        base_df : DataFrame de base (avec les variables cibles pour training)\n",
    "        source_dfs : dict de DataFrames à fusionner\n",
    "        merge_keys : colonnes de jointure\n",
    "    \n",
    "    Retourne:\n",
    "        DataFrame fusionné\n",
    "    \"\"\"\n",
    "    merged = base_df.copy()\n",
    "    \n",
    "    for name, df in source_dfs.items():\n",
    "        # Colonnes à ajouter (exclure les clés de jointure déjà présentes)\n",
    "        cols_to_add = [c for c in df.columns if c not in merged.columns]\n",
    "        \n",
    "        # Déterminer les clés de jointure disponibles\n",
    "        available_keys = [k for k in merge_keys if k in df.columns and k in merged.columns]\n",
    "        \n",
    "        if not available_keys:\n",
    "            print(f\"⚠️ {name}: pas de clés de jointure communes\")\n",
    "            continue\n",
    "        \n",
    "        if cols_to_add:\n",
    "            # Sélectionner les colonnes pour la jointure\n",
    "            df_subset = df[available_keys + cols_to_add].copy()\n",
    "            \n",
    "            # Fusionner\n",
    "            merged = merged.merge(\n",
    "                df_subset,\n",
    "                on=available_keys,\n",
    "                how='left'\n",
    "            )\n",
    "            print(f\"✅ {name}: +{len(cols_to_add)} colonnes (jointure sur {available_keys})\")\n",
    "        else:\n",
    "            print(f\"⏭️ {name}: pas de nouvelles colonnes\")\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "merge-training",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion des données TRAINING\n",
      "==================================================\n",
      "Base : (9319, 6)\n",
      "\n",
      "✅ landsat: +20 colonnes (jointure sur ['Latitude', 'Longitude', 'Sample Date'])\n",
      "✅ terraclimate: +34 colonnes (jointure sur ['Latitude', 'Longitude', 'Sample Date'])\n",
      "✅ worldcover: +11 colonnes (jointure sur ['Latitude', 'Longitude', 'Sample Date'])\n",
      "✅ soilgrids: +6 colonnes (jointure sur ['Latitude', 'Longitude', 'Sample Date'])\n",
      "✅ dem: +3 colonnes (jointure sur ['Latitude', 'Longitude', 'Sample Date'])\n",
      "✅ water_type: +2 colonnes (jointure sur ['Latitude', 'Longitude', 'Sample Date'])\n",
      "\n",
      "Résultat final : (9319, 82)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FUSION - TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Fusion des données TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Base : {raw_training.shape}\")\n",
    "print()\n",
    "\n",
    "merged_training = merge_all_sources(raw_training, training_dfs)\n",
    "\n",
    "print()\n",
    "print(f\"Résultat final : {merged_training.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "merge-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion des données VALIDATION\n",
      "==================================================\n",
      "Base : (200, 6)\n",
      "\n",
      "✅ landsat: +20 colonnes (jointure sur ['Latitude', 'Longitude', 'Sample Date'])\n",
      "✅ terraclimate: +34 colonnes (jointure sur ['Latitude', 'Longitude', 'Sample Date'])\n",
      "✅ worldcover: +11 colonnes (jointure sur ['Latitude', 'Longitude', 'Sample Date'])\n",
      "✅ soilgrids: +6 colonnes (jointure sur ['Latitude', 'Longitude', 'Sample Date'])\n",
      "✅ dem: +3 colonnes (jointure sur ['Latitude', 'Longitude', 'Sample Date'])\n",
      "✅ water_type: +2 colonnes (jointure sur ['Latitude', 'Longitude', 'Sample Date'])\n",
      "\n",
      "Résultat final : (200, 82)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FUSION - VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Fusion des données VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Base : {raw_validation.shape}\")\n",
    "print()\n",
    "\n",
    "merged_validation = merge_all_sources(raw_validation, validation_dfs)\n",
    "\n",
    "print()\n",
    "print(f\"Résultat final : {merged_validation.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Étape 3 : Vérification des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "check-missing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes (Training)\n",
      "==================================================\n",
      "\n",
      "27 colonnes avec valeurs manquantes:\n",
      "  soil_cec: 2918 (31.3%)\n",
      "  soil_sand: 2918 (31.3%)\n",
      "  soil_soc: 2918 (31.3%)\n",
      "  soil_nitrogen: 2918 (31.3%)\n",
      "  soil_clay: 2918 (31.3%)\n",
      "  soil_ph: 2918 (31.3%)\n",
      "  swir16: 100 (1.1%)\n",
      "  nir: 100 (1.1%)\n",
      "  blue: 100 (1.1%)\n",
      "  green: 100 (1.1%)\n",
      "  red: 100 (1.1%)\n",
      "  swir16_std: 100 (1.1%)\n",
      "  nir_std: 100 (1.1%)\n",
      "  red_std: 100 (1.1%)\n",
      "  green_std: 100 (1.1%)\n",
      "  blue_std: 100 (1.1%)\n",
      "  swir22: 100 (1.1%)\n",
      "  swir22_std: 100 (1.1%)\n",
      "  NDVI: 85 (0.9%)\n",
      "  NDMI_std: 85 (0.9%)\n",
      "  ... et 7 autres\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VÉRIFIER LES VALEURS MANQUANTES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Valeurs manquantes (Training)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing = merged_training.isnull().sum()\n",
    "missing_pct = (missing / len(merged_training) * 100).round(1)\n",
    "\n",
    "# Afficher seulement les colonnes avec des valeurs manquantes\n",
    "missing_cols = missing[missing > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"\\n{len(missing_cols)} colonnes avec valeurs manquantes:\")\n",
    "    for col in missing_cols.index[:20]:  # Top 20\n",
    "        print(f\"  {col}: {missing[col]} ({missing_pct[col]}%)\")\n",
    "    if len(missing_cols) > 20:\n",
    "        print(f\"  ... et {len(missing_cols) - 20} autres\")\n",
    "else:\n",
    "    print(\"Aucune valeur manquante !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "check-stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiques du dataset fusionné (Training)\n",
      "==================================================\n",
      "\n",
      "Dimensions : 9319 lignes x 82 colonnes\n",
      "\n",
      "Colonnes numériques : 80\n",
      "Colonnes catégorielles : 2\n",
      "\n",
      "Variables cibles :\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STATISTIQUES GLOBALES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Statistiques du dataset fusionné (Training)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nDimensions : {merged_training.shape[0]} lignes x {merged_training.shape[1]} colonnes\")\n",
    "\n",
    "# Compter les types de colonnes\n",
    "numeric_cols = merged_training.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = merged_training.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(f\"\\nColonnes numériques : {len(numeric_cols)}\")\n",
    "print(f\"Colonnes catégorielles : {len(cat_cols)}\")\n",
    "\n",
    "# Variables cibles\n",
    "target_cols = ['Alkalinity', 'Conductivity', 'Phosphorus']\n",
    "print(f\"\\nVariables cibles :\")\n",
    "for col in target_cols:\n",
    "    if col in merged_training.columns:\n",
    "        print(f\"  {col}: mean={merged_training[col].mean():.2f}, std={merged_training[col].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "list-features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features par source\n",
      "==================================================\n",
      "\n",
      "Landsat (18 features):\n",
      "  ['blue', 'green', 'red', 'nir', 'swir16'] ... ['NDWI', 'MNDWI', 'NDVI_std', 'NDWI_std', 'MNDWI_std']\n",
      "\n",
      "TerraClimate (38 features):\n",
      "  ['pet', 'aet', 'ppt', 'ppt_lag1', 'ppt_lag2'] ... ['soil_clay', 'soil_sand', 'soil_soc', 'soil_cec', 'soil_nitrogen']\n",
      "\n",
      "WorldCover (11 features):\n",
      "  ['lc_tree', 'lc_shrubland', 'lc_grassland', 'lc_cropland', 'lc_builtup'] ... ['lc_snow', 'lc_water', 'lc_wetland', 'lc_mangroves', 'lc_moss']\n",
      "\n",
      "SoilGrids (12 features):\n",
      "  ['soil_lag1', 'soil_lag2', 'soil_lag3', 'soil_sum4', 'soil_mean4'] ... ['soil_clay', 'soil_sand', 'soil_soc', 'soil_cec', 'soil_nitrogen']\n",
      "\n",
      "DEM (3 features):\n",
      "  ['elevation', 'slope', 'aspect']\n",
      "\n",
      "WaterType (1 features):\n",
      "  ['water_type']\n",
      "\n",
      "==================================================\n",
      "TOTAL : 83 features\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LISTE DES FEATURES PAR CATÉGORIE\n",
    "# =============================================================================\n",
    "\n",
    "# Colonnes de base (pas des features)\n",
    "base_cols = ['Latitude', 'Longitude', 'Sample Date', 'Alkalinity', 'Conductivity', 'Phosphorus']\n",
    "\n",
    "# Identifier les features par préfixe/source\n",
    "feature_groups = {\n",
    "    'Landsat': [c for c in merged_training.columns if any(x in c.lower() for x in ['blue', 'green', 'red', 'nir', 'swir', 'ndvi', 'ndwi', 'mndwi', 'ndti'])],\n",
    "    'TerraClimate': [c for c in merged_training.columns if any(x in c.lower() for x in ['ppt', 'tmax', 'tmin', 'soil', 'def', 'vpd', 'aet', 'pet', 'srad', 'swe'])],\n",
    "    'WorldCover': [c for c in merged_training.columns if c.startswith('lc_')],\n",
    "    'SoilGrids': [c for c in merged_training.columns if c.startswith('soil_')],\n",
    "    'DEM': [c for c in merged_training.columns if c in ['elevation', 'slope', 'aspect']],\n",
    "    'WaterType': [c for c in merged_training.columns if c == 'water_type'],\n",
    "}\n",
    "\n",
    "print(\"Features par source\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_features = 0\n",
    "for group, cols in feature_groups.items():\n",
    "    print(f\"\\n{group} ({len(cols)} features):\")\n",
    "    if len(cols) <= 10:\n",
    "        print(f\"  {cols}\")\n",
    "    else:\n",
    "        print(f\"  {cols[:5]} ... {cols[-5:]}\")\n",
    "    total_features += len(cols)\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"TOTAL : {total_features} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Étape 4 : Nettoyage (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "clean-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suppression de lc_snow (toujours 0)\n",
      "Suppression de lc_mangroves (toujours 0)\n",
      "Suppression de lc_moss (toujours 0)\n",
      "\n",
      "Dimensions après nettoyage : (9319, 79)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUPPRIMER LES COLONNES INUTILES\n",
    "# =============================================================================\n",
    "\n",
    "# Colonnes WorldCover toujours à 0 (pas présentes en Afrique du Sud)\n",
    "cols_to_drop = ['lc_snow', 'lc_mangroves', 'lc_moss']\n",
    "\n",
    "# Vérifier si ces colonnes existent et sont bien à 0\n",
    "for col in cols_to_drop:\n",
    "    if col in merged_training.columns:\n",
    "        if merged_training[col].sum() == 0:\n",
    "            print(f\"Suppression de {col} (toujours 0)\")\n",
    "            merged_training = merged_training.drop(columns=[col])\n",
    "            if col in merged_validation.columns:\n",
    "                merged_validation = merged_validation.drop(columns=[col])\n",
    "\n",
    "print(f\"\\nDimensions après nettoyage : {merged_training.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Étape 5 : Sauvegarder les datasets fusionnés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "save-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sauvegardé : ../data/processed\\merged_training.csv\n",
      "   9319 lignes x 79 colonnes\n",
      "\n",
      "✅ Sauvegardé : ../data/processed\\merged_validation.csv\n",
      "   200 lignes x 79 colonnes\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SAUVEGARDER LES FICHIERS\n",
    "# =============================================================================\n",
    "\n",
    "OUTPUT_DIR = \"../data/processed\"\n",
    "\n",
    "# Training\n",
    "train_path = os.path.join(OUTPUT_DIR, 'merged_training.csv')\n",
    "merged_training.to_csv(train_path, index=False)\n",
    "print(f\"✅ Sauvegardé : {train_path}\")\n",
    "print(f\"   {merged_training.shape[0]} lignes x {merged_training.shape[1]} colonnes\")\n",
    "\n",
    "# Validation\n",
    "val_path = os.path.join(OUTPUT_DIR, 'merged_validation.csv')\n",
    "merged_validation.to_csv(val_path, index=False)\n",
    "print(f\"\\n✅ Sauvegardé : {val_path}\")\n",
    "print(f\"   {merged_validation.shape[0]} lignes x {merged_validation.shape[1]} colonnes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "preview-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aperçu du dataset fusionné (Training)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Sample Date</th>\n",
       "      <th>Total Alkalinity</th>\n",
       "      <th>Electrical Conductance</th>\n",
       "      <th>Dissolved Reactive Phosphorus</th>\n",
       "      <th>blue</th>\n",
       "      <th>green</th>\n",
       "      <th>red</th>\n",
       "      <th>nir</th>\n",
       "      <th>...</th>\n",
       "      <th>soil_clay</th>\n",
       "      <th>soil_sand</th>\n",
       "      <th>soil_soc</th>\n",
       "      <th>soil_cec</th>\n",
       "      <th>soil_nitrogen</th>\n",
       "      <th>elevation</th>\n",
       "      <th>slope</th>\n",
       "      <th>aspect</th>\n",
       "      <th>water_type</th>\n",
       "      <th>distance_to_river</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-28.760833</td>\n",
       "      <td>17.730278</td>\n",
       "      <td>02-01-2011</td>\n",
       "      <td>128.912</td>\n",
       "      <td>555.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9824.479167</td>\n",
       "      <td>11602.008333</td>\n",
       "      <td>12700.020833</td>\n",
       "      <td>13351.962500</td>\n",
       "      <td>...</td>\n",
       "      <td>69.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>192.663025</td>\n",
       "      <td>11.798665</td>\n",
       "      <td>299.497650</td>\n",
       "      <td>river</td>\n",
       "      <td>46.592505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-26.861111</td>\n",
       "      <td>28.884722</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>74.720</td>\n",
       "      <td>162.9</td>\n",
       "      <td>163.0</td>\n",
       "      <td>8511.409524</td>\n",
       "      <td>9483.828571</td>\n",
       "      <td>8980.290476</td>\n",
       "      <td>19672.442857</td>\n",
       "      <td>...</td>\n",
       "      <td>302.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1527.916626</td>\n",
       "      <td>2.923243</td>\n",
       "      <td>109.644104</td>\n",
       "      <td>river</td>\n",
       "      <td>69.035778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-26.450000</td>\n",
       "      <td>28.085833</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>89.254</td>\n",
       "      <td>573.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9073.283898</td>\n",
       "      <td>9939.313559</td>\n",
       "      <td>11237.631356</td>\n",
       "      <td>13288.343220</td>\n",
       "      <td>...</td>\n",
       "      <td>260.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1473.671143</td>\n",
       "      <td>1.366939</td>\n",
       "      <td>134.574402</td>\n",
       "      <td>river</td>\n",
       "      <td>123.538285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-27.671111</td>\n",
       "      <td>27.236944</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>82.000</td>\n",
       "      <td>203.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>9790.723214</td>\n",
       "      <td>10862.504464</td>\n",
       "      <td>10937.803571</td>\n",
       "      <td>15800.040179</td>\n",
       "      <td>...</td>\n",
       "      <td>264.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1347.080688</td>\n",
       "      <td>3.807301</td>\n",
       "      <td>310.537842</td>\n",
       "      <td>river</td>\n",
       "      <td>20.518409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-27.356667</td>\n",
       "      <td>27.286389</td>\n",
       "      <td>03-01-2011</td>\n",
       "      <td>56.100</td>\n",
       "      <td>145.1</td>\n",
       "      <td>151.0</td>\n",
       "      <td>8812.910714</td>\n",
       "      <td>9734.102679</td>\n",
       "      <td>9446.839286</td>\n",
       "      <td>16401.950893</td>\n",
       "      <td>...</td>\n",
       "      <td>245.0</td>\n",
       "      <td>637.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1357.651001</td>\n",
       "      <td>1.690194</td>\n",
       "      <td>224.774612</td>\n",
       "      <td>river</td>\n",
       "      <td>46.165896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Latitude  Longitude Sample Date  Total Alkalinity  Electrical Conductance  \\\n",
       "0 -28.760833  17.730278  02-01-2011           128.912                   555.0   \n",
       "1 -26.861111  28.884722  03-01-2011            74.720                   162.9   \n",
       "2 -26.450000  28.085833  03-01-2011            89.254                   573.0   \n",
       "3 -27.671111  27.236944  03-01-2011            82.000                   203.6   \n",
       "4 -27.356667  27.286389  03-01-2011            56.100                   145.1   \n",
       "\n",
       "   Dissolved Reactive Phosphorus         blue         green           red  \\\n",
       "0                           10.0  9824.479167  11602.008333  12700.020833   \n",
       "1                          163.0  8511.409524   9483.828571   8980.290476   \n",
       "2                           80.0  9073.283898   9939.313559  11237.631356   \n",
       "3                          101.0  9790.723214  10862.504464  10937.803571   \n",
       "4                          151.0  8812.910714   9734.102679   9446.839286   \n",
       "\n",
       "            nir  ...  soil_clay  soil_sand  soil_soc  soil_cec  soil_nitrogen  \\\n",
       "0  13351.962500  ...       69.0      838.0     546.0     166.0          141.0   \n",
       "1  19672.442857  ...      302.0      540.0     161.0     200.0          200.0   \n",
       "2  13288.343220  ...      260.0      627.0     180.0     218.0          144.0   \n",
       "3  15800.040179  ...      264.0      616.0     162.0     195.0          129.0   \n",
       "4  16401.950893  ...      245.0      637.0     150.0     186.0          132.0   \n",
       "\n",
       "     elevation      slope      aspect  water_type  distance_to_river  \n",
       "0   192.663025  11.798665  299.497650       river          46.592505  \n",
       "1  1527.916626   2.923243  109.644104       river          69.035778  \n",
       "2  1473.671143   1.366939  134.574402       river         123.538285  \n",
       "3  1347.080688   3.807301  310.537842       river          20.518409  \n",
       "4  1357.651001   1.690194  224.774612       river          46.165896  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Aperçu final\n",
    "print(\"Aperçu du dataset fusionné (Training)\")\n",
    "display(merged_training.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Résumé\n",
    "\n",
    "**Fichiers créés :**\n",
    "\n",
    "| Fichier | Description |\n",
    "|---------|-------------|\n",
    "| `merged_training.csv` | Dataset complet pour l'entraînement |\n",
    "| `merged_validation.csv` | Dataset complet pour la soumission |\n",
    "\n",
    "**Prochaine étape :**\n",
    "- Entraîner le modèle avec toutes les nouvelles features\n",
    "- Comparer R² avant (~0.41) vs après\n",
    "- Tester XGBoost / LightGBM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
